{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "from torchvision.transforms import v2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models,transforms\n",
    "from transformers import pipeline\n",
    "import torchvision.transforms.functional as TF\n",
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CDC(nn.Module):\n",
    "    '''\n",
    "    This class performs central difference convolution (CDC) operation. First the normal convolution is performed and then the difference convolution is performed. The output is the difference between the two is taken.\n",
    "    '''\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1,\n",
    "                 padding=1, dilation=1, groups=1, bias=False, theta=0.7):\n",
    "\n",
    "        super(CDC, self).__init__()\n",
    "        self.bias= bias\n",
    "        self.stride = stride\n",
    "        self.groups = groups\n",
    "        self.dilation = dilation\n",
    "        self.theta = theta\n",
    "        self.padding = padding\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_channels))\n",
    "        else:\n",
    "            self.bias = None\n",
    "            \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding if kernel_size==3 else 0, dilation=dilation, groups=groups, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_normal = self.conv(x)\n",
    "        # if conv.weight is (out_channels, in_channels, kernel_size, kernel_size),\n",
    "        # then the  self.conv.weight.sum(2) will return (out_channels, in_channels,kernel_size)\n",
    "        # and self.conv.weight.sum(2).sum(2) will return (out_channels,n_channels)\n",
    "        kernel_diff = self.conv.weight.sum(2).sum(2)\n",
    "        # Here we are adding extra dimensions such that the kernel_diff is of shape (out_channels, in_channels, 1, 1) so that convolution can be performed.\n",
    "        kernel_diff = kernel_diff[:, :, None, None]\n",
    "        out_diff = F.conv2d(input=x, weight=kernel_diff, bias=self.bias, stride=self.stride, padding=0, groups=self.groups)\n",
    "        return out_normal - self.theta * out_diff\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class conv_block_nested(nn.Module):\n",
    "    def __init__(self, in_ch,  out_ch):\n",
    "        super(conv_block_nested, self).__init__()\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        self.conv1 = CDC(in_ch, out_ch, kernel_size=3, padding=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = CDC(out_ch, out_ch, kernel_size=3, padding=1, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        output = self.activation(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset): \n",
    "    \n",
    "    def __init__(self, path, device, transform=None, img_size=(128, 128)):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.device = device\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "        self.path = path\n",
    "        self.num_channels = 1\n",
    "        \n",
    "        for folder in os.listdir(self.path):\n",
    "            label = 1 if 'client' in folder else 0\n",
    "            for image in os.listdir(os.path.join(self.path, folder)):\n",
    "                if image.endswith('.jpg') or image.endswith('.png'):\n",
    "                    img_path = os.path.join(self.path, folder, image)\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(label)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.images[idx]).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img= self.transform(img)\n",
    "            \n",
    "        return img, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SimulateDistanceTransform:\n",
    "    def __init__(self, min_scale=0.5, max_scale=1.0):\n",
    "        self.min_scale = min_scale\n",
    "        self.max_scale = max_scale\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Randomly choose a scale factor\n",
    "        scale_factor = random.uniform(self.min_scale, self.max_scale)\n",
    "        \n",
    "        # Get original dimensions\n",
    "        original_width, original_height = img.size\n",
    "        \n",
    "        # Calculate new dimensions\n",
    "        new_width = int(original_width * scale_factor)\n",
    "        new_height = int(original_height * scale_factor)\n",
    "        \n",
    "        # Resize the image\n",
    "        img = transforms.Resize((new_height, new_width))(img)\n",
    "        \n",
    "        # Pad the image to the original size\n",
    "        padding = (\n",
    "            (original_width - new_width) // 2,\n",
    "            (original_height - new_height) // 2,\n",
    "            (original_width - new_width + 1) // 2,\n",
    "            (original_height - new_height + 1) // 2\n",
    "        )\n",
    "        img = transforms.Pad(padding,padding_mode='edge')(img)\n",
    "        \n",
    "        # Optional: Apply a slight blur\n",
    "        if scale_factor < 0.65:\n",
    "            img = transforms.GaussianBlur(kernel_size=3)(img)\n",
    "        \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "img_size = (252, 252)\n",
    "batch_size = 600\n",
    "\n",
    "transf = transforms.Compose([\n",
    "    SimulateDistanceTransform(min_scale=0.4, max_scale=1.0),  \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "#     transforms.CenterCrop(img_size),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "train_dataset = CustomDataset(\"/kaggle/input/increased-liveliness-detection/train\",device,transf,img_size=img_size)\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(\"/kaggle/input/increased-liveliness-detection/val\",device,transf,img_size=img_size)\n",
    "val_loader = DataLoader(val_dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "# test_dataset = CustomDataset(\"/kaggle/input/increased-liveliness-detection/test\",device,transf,img_size=img_size)\n",
    "# test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FineTuneDepthAnything(nn.Module):\n",
    "    def __init__(self, device,load_trained=False,model_path=None):\n",
    "        super(FineTuneDepthAnything, self).__init__()\n",
    "        if load_trained:\n",
    "            config = AutoConfig.from_pretrained(\"depth-anything/Depth-Anything-V2-Small-hf\")\n",
    "            self.depth_anything = AutoModelForDepthEstimation.from_config(config)\n",
    "            state_dict = torch.load(model_path, map_location=device)\n",
    "                \n",
    "            # Adjust keys in the state dictionary to match the model's keys\n",
    "            new_state_dict = {}\n",
    "            for key, value in state_dict.items():\n",
    "                new_key = key.replace(\"depth_anything.\", \"\")\n",
    "                new_state_dict[new_key] = value\n",
    "\n",
    "            # Load the adjusted state dictionary into the model\n",
    "            self.depth_anything.load_state_dict(new_state_dict)\n",
    "        else:\n",
    "            self.depth_anything = AutoModelForDepthEstimation.from_pretrained(\"depth-anything/Depth-Anything-V2-Small-hf\")\n",
    "            for name,param in self.depth_anything.named_parameters():\n",
    "                if 'head' in name or 'neck.fusion_stage.layers.2.residual_layer' in name or 'neck.fusion_stage.layers.3' in name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        self.depth_anything = self.depth_anything.to(device)\n",
    "                \n",
    "    def forward(self, inp):\n",
    "        # print(f'inp shape: {inp.shape}')\n",
    "        return self.depth_anything(inp).predicted_depth.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ClassifierUCDCN(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(ClassifierUCDCN, self).__init__()        \n",
    "        self.layers =8\n",
    "        self.dropout_prob = dropout\n",
    "        self.img_size = (252, 252)\n",
    "        self.hidden_size = 64\n",
    "        self.conv1 = conv_block_nested(1,self.layers)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.AvgPool2d(kernel_size=2,stride=2)\n",
    "        self.conv2 = conv_block_nested(self.layers,1)\n",
    "        # Maxpool\n",
    "        self.linear_1 = nn.Linear((self.img_size[0]//4 * self.img_size[1]//4), self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        self.linear_2 = nn.Linear(self.hidden_size, 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        conv1 = self.conv1(inp)\n",
    "        maxpool = self.maxpool(conv1)\n",
    "        conv2 = self.conv2(maxpool)\n",
    "        maxpool2 = self.maxpool(conv2)\n",
    "        linear_1 = self.linear_1(maxpool2.view(-1, self.img_size[0]//4 * self.img_size[1]//4))\n",
    "        relu = self.relu(linear_1)\n",
    "        dropout = self.dropout(relu)\n",
    "        linear_2 = self.linear_2(dropout)\n",
    "        return self.sigmoid(linear_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=0.27, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma  # Focusing parameter\n",
    "        self.size_average = size_average  # Whether to average the loss\n",
    "\n",
    "        # Handling the alpha parameter for balancing the classes\n",
    "        if isinstance(alpha, (float, int)):\n",
    "            self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
    "        elif isinstance(alpha, list):\n",
    "            self.alpha = torch.Tensor(alpha)\n",
    "        else:\n",
    "            self.alpha = None\n",
    "\n",
    "    def forward(self, inp, target):\n",
    "        # Reshape input if necessary\n",
    "        if inp.dim() > 2:\n",
    "            # Flatten the input except for the batch size\n",
    "            inp = inp.view(inp.size(0), inp.size(1), -1)  # N,C,H,W => N,C,H*W\n",
    "            inp = inp.transpose(1, 2)  # N,C,H*W => N,H*W,C\n",
    "            inp = inp.contiguous().view(-1, inp.size(2))  # N,H*W,C => N*H*W,C\n",
    "        \n",
    "        # Flatten target to match input dimensions\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        # Compute log probability\n",
    "        logpt = F.log_softmax(inp, dim=1)\n",
    "        logpt = logpt.gather(1, target)  # Gather the probabilities with respect to target labels\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = logpt.data.exp()  # Convert log probabilities to probabilities\n",
    "\n",
    "        # Apply alpha weighting\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != inp.data.type():\n",
    "                self.alpha = self.alpha.type_as(inp.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * at\n",
    "\n",
    "        # Compute the focal loss\n",
    "        loss = -1 * (1 - pt) ** self.gamma * logpt\n",
    "\n",
    "        # Return the average or sum of losses\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "depth_map_model = FineTuneDepthAnything(device, load_trained=True, model_path='/kaggle/input/finetune_depth_anything/pytorch/63_epochs_trained/1/fine_tuning_depth_anything.pth').to(device)\n",
    "depth_map_model = torch.nn.DataParallel(depth_map_model,device_ids=[0,1]).to(device)\n",
    "model = ClassifierUCDCN(dropout=0.4).to(device)\n",
    "model.load_state_dict(torch.load('/kaggle/input/finetune_depth_anyhting_classifier/pytorch/64_size_7_epochs_trained/1/64_finetune_depth_anything_classifier.pth', map_location=device,weights_only=True))\n",
    "model = torch.nn.DataParallel(model,device_ids=[0,1]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "criterion = FocalLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0015,weight_decay=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "best_epoch_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    total_correct = 0  # Total correct predictions\n",
    "    total_samples = 0  # Total samples processed\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, binary_labels = data\n",
    "        inputs, binary_labels = inputs.to(device), binary_labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            depth_maps = depth_map_model(inputs)\n",
    "        pred_labels = model(depth_maps)\n",
    "        loss = criterion(pred_labels, binary_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Update total correct predictions and total samples for accuracy calculation\n",
    "        total_correct += (pred_labels.argmax(1) == binary_labels).sum().item()\n",
    "        total_samples += binary_labels.size(0)\n",
    "\n",
    "    epoch_accuracy = total_correct / total_samples  # Calculate accuracy for the epoch\n",
    "    train_loss.append(running_loss)\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {running_loss} , Accuracy: {epoch_accuracy} and lr:{optimizer.param_groups[0]['lr']}\")\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            running_loss_test = 0.0\n",
    "            total_correct_test = 0  # Total correct predictions for validation\n",
    "            total_samples_test = 0  # Total samples processed for validation\n",
    "            for i, data in enumerate(val_loader, 0):\n",
    "                inputs_test, binary_labels_test = data\n",
    "                inputs_test, binary_labels_test = inputs_test.to(device), binary_labels_test.to(device)\n",
    "                depth_maps_test = depth_map_model(inputs_test)\n",
    "                pred_labels_test = model(depth_maps_test)\n",
    "                loss_test = criterion(pred_labels_test, binary_labels_test)\n",
    "                running_loss_test += loss_test.item()\n",
    "\n",
    "                # Update total correct predictions and total samples for validation accuracy calculation\n",
    "                total_correct_test += (pred_labels_test.argmax(1) == binary_labels_test).sum().item()\n",
    "                total_samples_test += binary_labels_test.size(0)\n",
    "\n",
    "            epoch_accuracy_test = total_correct_test / total_samples_test  # Calculate accuracy for the validation epoch\n",
    "            val_loss.append(running_loss_test)\n",
    "            print(f\"Validation Loss: {running_loss_test} , Accuracy: {epoch_accuracy_test} and lr:{optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "            if running_loss_test < best_epoch_loss:\n",
    "                best_epoch_loss = running_loss_test\n",
    "                torch.save(model.module.state_dict(), \"best_finetune_depth_anything_classifier.pth\")\n",
    "\n",
    "    torch.save(model.module.state_dict(), \"finetune_depth_anything_classifier.pth\")\n",
    "    scheduler.step(running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6345867,
     "sourceId": 10258327,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 103067,
     "modelInstanceId": 176431,
     "sourceId": 206950,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 106709,
     "modelInstanceId": 177671,
     "sourceId": 208397,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
