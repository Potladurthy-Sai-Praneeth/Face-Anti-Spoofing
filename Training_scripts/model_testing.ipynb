{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T07:39:11.666766Z",
     "iopub.status.busy": "2024-12-24T07:39:11.666441Z",
     "iopub.status.idle": "2024-12-24T07:39:31.916857Z",
     "shell.execute_reply": "2024-12-24T07:39:31.916037Z",
     "shell.execute_reply.started": "2024-12-24T07:39:11.666741Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "from torchvision.transforms import v2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models,transforms\n",
    "from transformers import pipeline\n",
    "import torchvision.transforms.functional as TF\n",
    "# import timm\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# from face_recognition import face_locations, face_encodings, compare_faces\n",
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T07:39:31.918946Z",
     "iopub.status.busy": "2024-12-24T07:39:31.918305Z",
     "iopub.status.idle": "2024-12-24T07:39:31.926564Z",
     "shell.execute_reply": "2024-12-24T07:39:31.925482Z",
     "shell.execute_reply.started": "2024-12-24T07:39:31.918912Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CDC(nn.Module):\n",
    "    '''\n",
    "    This class performs central difference convolution (CDC) operation. First the normal convolution is performed and then the difference convolution is performed. The output is the difference between the two is taken.\n",
    "    '''\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1,\n",
    "                 padding=1, dilation=1, groups=1, bias=False, theta=0.7):\n",
    "\n",
    "        super(CDC, self).__init__()\n",
    "        self.bias= bias\n",
    "        self.stride = stride\n",
    "        self.groups = groups\n",
    "        self.dilation = dilation\n",
    "        self.theta = theta\n",
    "        self.padding = padding\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_channels))\n",
    "        else:\n",
    "            self.bias = None\n",
    "            \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding if kernel_size==3 else 0, dilation=dilation, groups=groups, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_normal = self.conv(x)\n",
    "        # if conv.weight is (out_channels, in_channels, kernel_size, kernel_size),\n",
    "        # then the  self.conv.weight.sum(2) will return (out_channels, in_channels,kernel_size)\n",
    "        # and self.conv.weight.sum(2).sum(2) will return (out_channels,n_channels)\n",
    "        kernel_diff = self.conv.weight.sum(2).sum(2)\n",
    "        # Here we are adding extra dimensions such that the kernel_diff is of shape (out_channels, in_channels, 1, 1) so that convolution can be performed.\n",
    "        kernel_diff = kernel_diff[:, :, None, None]\n",
    "        out_diff = F.conv2d(input=x, weight=kernel_diff, bias=self.bias, stride=self.stride, padding=0, groups=self.groups)\n",
    "        return out_normal - self.theta * out_diff\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T07:39:31.928134Z",
     "iopub.status.busy": "2024-12-24T07:39:31.927776Z",
     "iopub.status.idle": "2024-12-24T07:39:31.962887Z",
     "shell.execute_reply": "2024-12-24T07:39:31.962226Z",
     "shell.execute_reply.started": "2024-12-24T07:39:31.928105Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class conv_block_nested(nn.Module):\n",
    "    def __init__(self, in_ch,  out_ch):\n",
    "        super(conv_block_nested, self).__init__()\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        self.conv1 = CDC(in_ch, out_ch, kernel_size=3, padding=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = CDC(out_ch, out_ch, kernel_size=3, padding=1, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        output = self.activation(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T07:39:31.965466Z",
     "iopub.status.busy": "2024-12-24T07:39:31.965147Z",
     "iopub.status.idle": "2024-12-24T07:39:31.977120Z",
     "shell.execute_reply": "2024-12-24T07:39:31.976516Z",
     "shell.execute_reply.started": "2024-12-24T07:39:31.965438Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset): \n",
    "    \n",
    "    def __init__(self, path, device, transform=None, img_size=(128, 128)):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.device = device\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "        self.path = path\n",
    "        self.num_channels = 1\n",
    "        \n",
    "        for folder in os.listdir(self.path):\n",
    "            label = 1 if 'client' in folder else 0\n",
    "            for image in os.listdir(os.path.join(self.path, folder)):\n",
    "                if image.endswith('.jpg') or image.endswith('.png'):\n",
    "                    img_path = os.path.join(self.path, folder, image)\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(label)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.images[idx]).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img= self.transform(img)\n",
    "            \n",
    "        return img, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T07:39:31.978336Z",
     "iopub.status.busy": "2024-12-24T07:39:31.978079Z",
     "iopub.status.idle": "2024-12-24T07:39:32.206940Z",
     "shell.execute_reply": "2024-12-24T07:39:32.206075Z",
     "shell.execute_reply.started": "2024-12-24T07:39:31.978317Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "img_size = (252, 252)\n",
    "batch_size = 600\n",
    "transf = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# train_dataset = CustomDataset(\"F:/Lecture Notes/Sample_projects/Face_recognition/Split_processed_dataset/train\",device,transf,img_size=img_size)\n",
    "# train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "# val_dataset = CustomDataset(\"/kaggle/input/liveliness-data/Split_processed_dataset/val\",device,transf,img_size=img_size)\n",
    "# val_loader = DataLoader(val_dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(\"/kaggle/input/increased-liveliness-detection/test\",device,transf,img_size=img_size)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T07:39:32.208504Z",
     "iopub.status.busy": "2024-12-24T07:39:32.208155Z",
     "iopub.status.idle": "2024-12-24T07:39:32.215942Z",
     "shell.execute_reply": "2024-12-24T07:39:32.215098Z",
     "shell.execute_reply.started": "2024-12-24T07:39:32.208464Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FineTuneDepthAnything(nn.Module):\n",
    "    def __init__(self, device,load_trained=False,model_path=None):\n",
    "        super(FineTuneDepthAnything, self).__init__()\n",
    "        if load_trained:\n",
    "            config = AutoConfig.from_pretrained(\"depth-anything/Depth-Anything-V2-Small-hf\")\n",
    "            self.depth_anything = AutoModelForDepthEstimation.from_config(config)\n",
    "            state_dict = torch.load(model_path, map_location=device)\n",
    "                \n",
    "            # Adjust keys in the state dictionary to match the model's keys\n",
    "            new_state_dict = {}\n",
    "            for key, value in state_dict.items():\n",
    "                new_key = key.replace(\"depth_anything.\", \"\")\n",
    "                new_state_dict[new_key] = value\n",
    "\n",
    "            # Load the adjusted state dictionary into the model\n",
    "            self.depth_anything.load_state_dict(new_state_dict)\n",
    "        else:\n",
    "            self.depth_anything = AutoModelForDepthEstimation.from_pretrained(\"depth-anything/Depth-Anything-V2-Small-hf\")\n",
    "            for name,param in self.depth_anything.named_parameters():\n",
    "                if 'head' in name or 'neck.fusion_stage.layers.2.residual_layer' in name or 'neck.fusion_stage.layers.3' in name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        self.depth_anything = self.depth_anything.to(device)\n",
    "                \n",
    "    def forward(self, inp):\n",
    "        # print(f'inp shape: {inp.shape}')\n",
    "        return self.depth_anything(inp).predicted_depth.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T07:39:32.217321Z",
     "iopub.status.busy": "2024-12-24T07:39:32.217020Z",
     "iopub.status.idle": "2024-12-24T07:39:32.229320Z",
     "shell.execute_reply": "2024-12-24T07:39:32.228476Z",
     "shell.execute_reply.started": "2024-12-24T07:39:32.217291Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ClassifierUCDCN(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(ClassifierUCDCN, self).__init__()\n",
    "        self.layers =8\n",
    "        self.dropout_prob = dropout\n",
    "        self.img_size = (252, 252)\n",
    "        self.hidden_size = 64\n",
    "        self.conv1 = conv_block_nested(1,self.layers)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.AvgPool2d(kernel_size=2,stride=2)\n",
    "        self.conv2 = conv_block_nested(self.layers,1)\n",
    "        # Maxpool\n",
    "        self.linear_1 = nn.Linear((self.img_size[0]//4 * self.img_size[1]//4), self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        self.linear_2 = nn.Linear(self.hidden_size, 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        conv1 = self.conv1(inp)\n",
    "        maxpool = self.maxpool(conv1)\n",
    "        conv2 = self.conv2(maxpool)\n",
    "        maxpool2 = self.maxpool(conv2)\n",
    "        linear_1 = self.linear_1(maxpool2.view(-1, self.img_size[0]//4 * self.img_size[1]//4))\n",
    "        relu = self.relu(linear_1)\n",
    "        dropout = self.dropout(relu)\n",
    "        linear_2 = self.linear_2(dropout)\n",
    "        return self.sigmoid(linear_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T07:39:32.230574Z",
     "iopub.status.busy": "2024-12-24T07:39:32.230329Z",
     "iopub.status.idle": "2024-12-24T07:39:32.241221Z",
     "shell.execute_reply": "2024-12-24T07:39:32.240558Z",
     "shell.execute_reply.started": "2024-12-24T07:39:32.230551Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_batch(batch,device,size=(224,224)):\n",
    "    batch = batch / 255.0 if batch.max() > 1 else batch\n",
    "    \n",
    "    # Resize the batch\n",
    "    resized = F.interpolate(batch, size=size, mode='bilinear', align_corners=False)\n",
    "    \n",
    "    # Repeat the single channel to create an RGB image\n",
    "    rgb_batch = resized.repeat(1, 3, 1, 1)\n",
    "    \n",
    "    # Move the batch to the specified device\n",
    "    rgb_batch = rgb_batch.to(device)\n",
    "    \n",
    "    return rgb_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T07:39:32.242371Z",
     "iopub.status.busy": "2024-12-24T07:39:32.242151Z",
     "iopub.status.idle": "2024-12-24T07:39:34.668336Z",
     "shell.execute_reply": "2024-12-24T07:39:34.667491Z",
     "shell.execute_reply.started": "2024-12-24T07:39:32.242353Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "depth_map_model = FineTuneDepthAnything(device, load_trained=True, model_path='/kaggle/input/finetune_depth_anything/pytorch/63_epochs_trained/1/fine_tuning_depth_anything.pth').to(device)\n",
    "depth_map_model = torch.nn.DataParallel(depth_map_model,device_ids=[0,1]).to(device)\n",
    "model = ClassifierUCDCN(dropout=0.5).to(device)\n",
    "model.load_state_dict(torch.load('/kaggle/input/finetune_depth_anyhting_classifier/pytorch/64_size_7_epochs_trained/1/64_finetune_depth_anything_classifier.pth', map_location=device))\n",
    "model = torch.nn.DataParallel(model,device_ids=[0,1]).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T07:39:34.670928Z",
     "iopub.status.busy": "2024-12-24T07:39:34.670662Z",
     "iopub.status.idle": "2024-12-24T07:40:45.557665Z",
     "shell.execute_reply": "2024-12-24T07:40:45.556783Z",
     "shell.execute_reply.started": "2024-12-24T07:39:34.670907Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        depth_maps = depth_map_model(images)\n",
    "        # Get model outputs\n",
    "        outputs = model(depth_maps)\n",
    "        predicted = torch.argmax(outputs, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the network on the test images: {accuracy:.2f} %')\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6345867,
     "sourceId": 10258327,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 103067,
     "modelInstanceId": 176431,
     "sourceId": 206950,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 106709,
     "modelInstanceId": 177671,
     "sourceId": 208397,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
